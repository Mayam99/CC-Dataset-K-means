# -*- coding: utf-8 -*-
"""CC-Dataset-K-means.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k-iKkwLMFG2ndbk-sr79hFTReaQhC-6Y

#About the Dataset
###The Credit Card Dataset for clustering is a popular dataset used to perform clustering analysis. One widely known version of this dataset is available on Kaggle. It contains anonymized data about credit card transactions and various attributes that can be used to understand customer behavior.

###Source: You can find the dataset on Kaggle at the following link:

###Kaggle: Credit Card Dataset for Clustering

#Introduction:

The Credit Card Dataset for Clustering consists of data about customers' credit card transactions and behavior. This dataset can be used to perform clustering analysis to segment customers based on their spending patterns and other attributes. The goal of clustering in this context is to group customers into different segments that exhibit similar behavior, which can then be used for targeted marketing, risk management, and improving customer service.

The dataset typically contains the following features:



*   Balance: The balance amount on the credit card.
*   Balance Frequency: Frequency of balance updates.
* Purchases: Total amount of purchases made.
* One-off Purchases: Amount of one-off purchases.
* Installments Purchases: Amount of installment purchases.
* Cash Advance: Amount of cash advances taken.
* Purchases Frequency: Frequency of purchases.
* One-off Purchases Frequency: Frequency of one-off purchases.
* Purchases Installments Frequency: Frequency of installment purchases.
* Cash Advance Frequency: Frequency of cash advances.
* Cash Advance Trx: Number of cash advance transactions.
* Purchases Trx: Number of purchase transactions.
* Credit Limit: Credit limit assigned to the customer.
* Payments: Total amount of payments made.
* Minimum Payments: Minimum payments made.
* PRC Full Payment: Percent of full paymen
* Tenure: Tenure of credit card service.

#Importing Necessary Libraries

We are required to importing the libraries so as to performing EDA. These include NumPy, Pandas, Matplotlib, and Seaborn.
"""

import pandas as pd # Pandas is a powerful library for data manipulation and analysis.
import numpy as np # NumPy is a powerful tool for numerical computations in Python.
from sklearn import cluster

"""#Loading the Dataset
The line df = pd.read_csv starts the process of reading a CSV file into a pandas DataFrame.

This allows for easy data manipulation and analysis using pandas functionalities.
"""

cc = pd.read_csv("CC GENERAL.csv")

cc  # Displays the first 5 rows and the last 5 rows of the Dataset

cc.columns # Displays the names of the columns

cc.shape # Displays the total count of the Rows and Columns of the dataset respectively.

cc.describe()

"""The df.info() method in pandas provides a concise summary of a DataFrame.
Purpose:

* Overview: Gives an overview of the DataFrameâ€™s structure and contents.

* Data Types: Displays the data types of each column.

* Non-null Values: Shows the number of non-null (non-missing) values in each column.

* Memory Usage: Indicates the memory usage of the DataFrame.
"""

cc.info()

"""# Checking if there is any Null value in the Data

We will now check for missing values in our dataset. In case there are any missing entries, we will impute them with appropriate values (mode in case of categorical feature, and median or mean in case of numerical feature). We will use the isnull() function for this purpose.
"""

cc.isnull().sum() # Displays the total count of the null valuesin the particular columns.

"""There are very few null values in credit limit and minimum payments column. Lets impute these values later."""

cc = cc.drop(['MINIMUM_PAYMENTS', 'CREDIT_LIMIT'], axis=1) #Dropping the unwanted cloumns

cc.isnull().sum() #Checking the dataframe for missing or null values

cc.dtypes

cc= cc.drop(['CUST_ID'], axis=1) #Dropping the column which is not needed

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() #Initialize the scaler: Create an instance of StandardScaler.
scaled_cc = scaler.fit_transform(cc) # Fit the scaler to the selected columns and transform them

cc = pd.get_dummies(cc, drop_first=True) #Used for encoding categorical variables in the dataset

cc = cc.dropna() #Used for dropping nan values

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = [] # This list will store the WCSS values for different numbers of clusters.
for i in range(1, 11): #This loop will calculate the WCSS for each number of clusters from 1 to 10.
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42) # Initialize k-means with i clusters:
    kmeans.fit(scaled_cc) #This trains the k-means algorithm on the scaled dataset.
    wcss.append(kmeans.inertia_) #The inertia_ attribute of the fitted k-means object gives the WCSS, which is then appended to the list.

#Plot the WCSS values:
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

from sklearn.metrics import silhouette_score

for n_clusters in range(2, 11): #This loop will calculate the Silhouette Score for each number of clusters from 2 to 10.
    kmeans = KMeans(n_clusters=n_clusters, random_state=42) #Initialize k-means with n_clusters clusters:
    cluster_labels = kmeans.fit_predict(scaled_cc) #Fit the k-means algorithm to the scaled data and predict cluster labels
    silhouette_avg = silhouette_score(scaled_cc, cluster_labels) #silhouette_score function computes the average Silhouette Score for all samples. This score provides an indication of how well the samples have been clustered.
    print(f'For n_clusters = {n_clusters}, the average silhouette_score is {silhouette_avg}') #Print the Silhouette Score.

kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(scaled_cc)
cc['Cluster'] = cluster_labels

print(kmeans.cluster_centers_)

import seaborn as sns

sns.pairplot(cc, hue='TENURE')
plt.show()